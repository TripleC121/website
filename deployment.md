# Deployment Guide

This document outlines the deployment process for Chris Chesley's personal website.

## Prerequisites

- AWS account with EC2 and S3 access
- Docker and Docker Compose installed on the EC2 instance
- GitHub account with access to the repository

## Deployment Steps

1. Set up an EC2 instance with Ubuntu
2. Install Docker and Docker Compose on the EC2 instance
3. Set up an S3 bucket for static files and images
4. Configure the EC2 security group to allow inbound traffic on ports 80 and 443
5. Set up Nginx on the EC2 instance as a reverse proxy
6. Clone the repository on the EC2 instance
7. Create a `.env.prod` file with production environment variables
8. Build and start the Docker containers:
   ```
   docker compose -f
   docker compose.prod.yml up -d
   ```

9. Run migrations:
   ```
   docker compose -f
   docker compose.prod.yml exec web python manage.py migrate
   ```

10. Collect static files:
    ```
    docker compose -f
    docker compose.prod.yml exec web python manage.py collectstatic
    ```

11. Optimize and sync images to S3:
    docker compose -f
    docker compose.prod.yml exec web python scripts/optimize_images.py

This script will:
- Process images in the input directory
- Create JPEG and WebP versions of each image
- Ensure all processed images are under 500 KB
- Move original images to a preservation directory
- Log the process for debugging purposes

## Continuous Deployment

The project uses GitHub Actions for continuous deployment. The workflows are defined in `.github/workflows/`:

- `deploy-ssm.yml`: Deploys the application to EC2
- `test-lint.yml`: Runs tests and linting
- `s3-sync.yml`: Syncs static files and images to S3

To set up continuous deployment:

1. Add the following secrets to your GitHub repository:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
   - `EC2_INSTANCE_ID`
   - `AWS_S3_BUCKET_NAME`
2. Ensure the EC2 instance has the AWS Systems Manager agent installed
3. Configure the EC2 instance's IAM role to allow Systems Manager access

Now, every push to the `main` branch will trigger the deployment workflows.

## Image Handling

The project uses WebP images for improved performance. The `optimize_images.py` script handles image optimization:

1. Place new images in the input directory specified in your `.env.prod` file
2. Run the optimization script as part of the deployment process
3. The S3 sync workflow will automatically upload the optimized images to S3

## Troubleshooting

- Check the EC2 instance's system log for deployment errors
- Verify that the S3 bucket permissions are correctly set
- Ensure that the EC2 instance has the necessary permissions to pull from the Docker registry and access S3
- For image-related issues, check the logs generated by the `optimize_images.py` script

For any persistent issues, please open an issue on the GitHub repository.
